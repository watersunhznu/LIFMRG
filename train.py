# coding=utf-8
import os
import torch
import argparse
from models import EncoderCNN, SentenceLSTM, WordLSTM, MLC, Tag_fuse
from dataloader import get_loader
from score import evalscores
from torchvision import transforms
from torch import nn
import numpy as np
from nltk.translate.bleu_score import corpus_bleu
import pandas
import visdom
import metrics
from focal_loss import focal_loss


def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    # random.seed(seed)
    # torch.backends.cudnn.deterministic = True


def script(args):
    transform = transforms.Compose([
        transforms.Resize(args.img_size),
        transforms.RandomCrop(args.crop_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()])

    if args.dataset == 'iuxray':
        tags_l = ['normal', 'opacity', 'degenerative change', 'atelectases', 'atelectasis', 'scarring',
                  'cardiomegaly', 'calcified granuloma', 'granuloma', 'pneumonia', 'pleural effusion', 'sternotomy',
                  'pleural effusions', 'pulmonary emphysema', 'infiltrates', 'emphysemas', 'granulomatous disease',
                  'nodule', 'pulmonary edema', 'diaphragm', 'emphysema', 'deformity', 'thoracic aorta',
                  'osteophytes', 'hiatal hernia', 'thoracic vertebrae', 'fracture', 'tortuous aorta',
                  'bilateral pleural effusion', 'rib fracture', 'aorta', 'edemas', 'calcinosis', 'scar', 'edema',
                  'copd', 'pulmonary disease, chronic obstructive', 'pneumothorax', 'effusion', 'pleural thickening',
                  'pulmonary atelectasis', 'congestion', 'eventration', 'rib fractures', 'hyperinflation',
                  'arthritic changes', 'ribs', 'cabg', 'catheterization, central venous', 'infection', 'others']
    elif args.dataset == 'mimic':
        tags_l = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Lesion',
                  'Lung Opacity', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax',
                  'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']
    else:
        raise Exception('dataset without support')
    train_loader, vocab = get_loader(args.root_dir,
                                     args.train_tsv_path,
                                     args.image_path,
                                     transform,
                                     args.batch_size,
                                     args.shuffle,
                                     args.num_workers,
                                     args.dataset)
    for tag in tags_l:
        vocab.add_word(tag)
    vocab_size = len(vocab)
    print("vocab_size: ", vocab_size)

    val_loader, _ = get_loader(args.root_dir,
                               args.val_tsv_path,
                               args.image_path,
                               transform,
                               args.batch_size,
                               args.shuffle,
                               args.num_workers,
                               args.dataset,
                               vocab)

    title = args.title
    loss_t = visdom.Visdom(env=title)
    scores_t = visdom.Visdom(env=title)

    loss_t.line([[0., 0.]], [0.], win='loss', opts=dict(title='loss', legend=['train', 'val']))
    scores_t.line([[0., 0., 0., 0., 0., 0., 0.]], [0.], win='scores',
                  opts=dict(title='scores', legend=['blue1', 'blue2', 'blue3', 'blue4', 'meteor', 'rouge', 'cider']))

    encoderCNN = EncoderCNN().to(args.device)

    mlc = MLC(tags_l, 256, encoderCNN.enc_dim, vocab_size, args.device).to(args.device)

    sentLSTM = SentenceLSTM(encoderCNN.enc_dim, args.sent_hidden_dim, args.att_dim, args.sent_input_dim,
                            args.word_input_dim, args.int_stop_dim).to(args.device)

    wordLSTM = WordLSTM(args.word_input_dim, args.word_hidden_dim, vocab_size, args.num_layers).to(
        args.device)

    # criterion_tags = nn.CrossEntropyLoss().to(args.device)
    criterion_stop = nn.CrossEntropyLoss().to(args.device)
    criterion_words = nn.CrossEntropyLoss().to(args.device)
    criterion_fl = focal_loss(alpha=args.fl_alpha, gamma=args.fl_gamma).to(args.device)
    # criterion_mse = nn.MSELoss().to(args.device)

    params_cnn = list(encoderCNN.parameters())
    params_mlc = list(mlc.parameters())
    params_lstm = list(sentLSTM.parameters()) + list(wordLSTM.parameters())

    optim_cnn = torch.optim.Adam(params=params_cnn, lr=args.learning_rate_cnn)  # , weight_decay=1e-4)
    optim_mlc = torch.optim.Adam(params=params_mlc, lr=args.learning_rate_mlc)  # L2 regularization
    optim_lstm = torch.optim.Adam(params=params_lstm, lr=args.learning_rate_lstm)

    total_step = len(train_loader)

    evaluate(args, val_loader, encoderCNN, mlc, sentLSTM, wordLSTM, vocab, 0, '', tags_l)

    for epoch in range(args.num_epochs):
        encoderCNN.train()
        mlc.train()
        sentLSTM.train()
        wordLSTM.train()
        met_t = np.zeros([len(tags_l), 4])
        t_tag_l, t_sent_l, t_word_l, t_l = 0, 0, 0, 0

        for i, (images, captions, prob, tags) in enumerate(train_loader):
            optim_cnn.zero_grad()
            optim_mlc.zero_grad()
            optim_lstm.zero_grad()
            # l1_regularization = torch.tensor([0], dtype=torch.float32).to(args.device)

            batch_size = images.shape[0]
            images = images.to(args.device)
            captions = captions.to(args.device)
            prob = prob.to(args.device)
            tags = tags.to(args.device)

            vis_enc_output = encoderCNN(images)
            sem_enc_output = vis_enc_output[:, :1, :]
            vis_enc_output = vis_enc_output[:, 1:, :]
            # sem_enc_output = vis_enc_output.mean(1).unsqueeze(1)
            tag_score, sem_enc_output, _1 = mlc(sem_enc_output, args.device)
            topics, ps = sentLSTM(vis_enc_output, sem_enc_output, captions, args.device)
            met_t = metrics.cala(tags, _1, met_t)

            # for parm in mlc.parameters():
            #     l1_regularization += torch.sum(abs(parm))   calculation L1 regularization

            # tags_loss = criterion_tags(tag_score.view(-1, 2), tags.view(-1))
            tags_loss = criterion_fl(tag_score.view(-1, 2), tags.view(-1))
            # tags_loss = criterion_mse(tag_score.view(-1), tags.view(-1).float())
            loss_sent = criterion_stop(ps.view(-1, 2), prob.view(-1))

            loss_word = torch.tensor([0.0]).to(args.device)
            for j in range(captions.shape[1]):
                word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :])

                loss_word += criterion_words(word_outputs.contiguous().view(-1, vocab_size),
                                             captions[:, j, :].contiguous().view(-1))

            # loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag \
            #        + args.lambda_l1 * l1_regularization
            loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag
            # loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word
            t_tag_l += tags_loss.item()
            t_sent_l += loss_sent.item()
            t_word_l += loss_word.item()
            t_l = t_tag_l + t_sent_l + t_word_l

            loss.backward()
            optim_cnn.step()
            optim_mlc.step()
            optim_lstm.step()

            # Print log info
            if i % args.log_step == 0:
                print('train： Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} >{:.2f}< >{:.2f}< >{:.5f}<'
                    .format(
                    epoch, args.num_epochs, i, total_step, loss.item(), t_sent_l * args.lambda_sent / (i + 1),
                                                                        t_word_l * args.lambda_word / (i + 1),
                                                                        t_tag_l * args.lambda_tag / (i + 1)))
        ## Save the model checkpoints
        # if (i+1) % args.save_step == 0:
        #     torch.save(decoder.state_dict(), os.path.join(
        #         args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))
        #     torch.save(encoder.state_dict(), os.path.join(
        #         args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))

        scores = evaluate(args, val_loader, encoderCNN, mlc, sentLSTM, wordLSTM, vocab, epoch, title, tags_l)
        loss_t.line([[t_l / (i + 1), scores[0]]], [epoch], win='loss', update='append')
        scores_t.line([scores[1:]], [epoch], win='scores', update='append')

        print('train metrics', np.sum(met_t, axis=0), 'Acc Precision Recall F1---------------->>',
              metrics.acc_f1(met_t))
    for i in range(len(tags_l)):
        print(' ' * 8, tags_l[i], ' ' * (40 - len(tags_l[i])), metrics.acc_f1(met_t[i]), met_t[i])


def main(args):
    transform = transforms.Compose([
        transforms.Resize(args.img_size),
        transforms.RandomCrop(args.crop_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()])

    if args.dataset == 'iuxray':
        tags_l = ['normal', 'opacity', 'degenerative change', 'atelectases', 'atelectasis', 'scarring',
                  'cardiomegaly', 'calcified granuloma', 'granuloma', 'pneumonia', 'pleural effusion', 'sternotomy',
                  'pleural effusions', 'pulmonary emphysema', 'infiltrates', 'emphysemas', 'granulomatous disease',
                  'nodule', 'pulmonary edema', 'diaphragm', 'emphysema', 'deformity', 'thoracic aorta',
                  'osteophytes', 'hiatal hernia', 'thoracic vertebrae', 'fracture', 'tortuous aorta',
                  'bilateral pleural effusion', 'rib fracture', 'aorta', 'edemas', 'calcinosis', 'scar', 'edema',
                  'copd', 'pulmonary disease, chronic obstructive', 'pneumothorax', 'effusion', 'pleural thickening',
                  'pulmonary atelectasis', 'congestion', 'eventration', 'rib fractures', 'hyperinflation',
                  'arthritic changes', 'ribs', 'cabg', 'catheterization, central venous', 'infection', 'others']
    elif args.dataset == 'mimic':
        tags_l = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Lesion',
                  'Lung Opacity', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax',
                  'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']
    elif args.dataset == 'bra':
        tags_l = ['淋巴结增多', '混合型', '片絮状影', '结节影', '腺体影', '腺体致密', '致密灶', '轻度增大', '钙化灶', '高密度影']
    else:
        raise Exception('dataset without support')
    train_loader, vocab = get_loader(args.root_dir,
                                     args.train_tsv_path,
                                     args.image_path,
                                     transform,
                                     args.batch_size,
                                     args.shuffle,
                                     args.num_workers,
                                     args.dataset)
    for tag in tags_l:
        vocab.add_word(tag)
    vocab_size = len(vocab)
    print("vocab_size: ", vocab_size)

    val_loader, _ = get_loader(args.root_dir,
                               args.val_tsv_path,
                               args.image_path,
                               transform,
                               args.batch_size,
                               args.shuffle,
                               args.num_workers,
                               args.dataset,
                               vocab)

    title = args.title
    loss_t = visdom.Visdom(env=title)
    scores_t = visdom.Visdom(env=title)

    loss_t.line([[0., 0.]], [0.], win='loss', opts=dict(title='loss', legend=['train', 'val']))
    scores_t.line([[0., 0., 0., 0., 0., 0., 0.]], [0.], win='scores',
                  opts=dict(title='scores', legend=['blue1', 'blue2', 'blue3', 'blue4', 'meteor', 'rouge', 'cider']))

    model = Tag_fuse(tags_l, vocab_size, args).to(args.device)
    # model = nn.DataParallel(model)

    # criterion_tags = nn.CrossEntropyLoss().to(args.device)
    criterion_stop = nn.CrossEntropyLoss().to(args.device)
    criterion_words = nn.CrossEntropyLoss().to(args.device)
    criterion_fl = focal_loss(alpha=args.fl_alpha, gamma=args.fl_gamma).to(args.device)
    # criterion_mse = nn.MSELoss().to(args.device)
    # params_model = list(model.parameters())
    params_dicts = [
        {'params': model.encoderCNN.parameters(), 'lr': args.learning_rate_cnn},
        {'params': model.mlc.parameters(), 'lr': args.learning_rate_mlc},
        {'params': model.sentLSTM.parameters(), 'lr': args.learning_rate_mlc},
        {'params': model.wordLSTM.parameters(), 'lr': 1e-3}
    ]

    optim_model = torch.optim.Adam(params=params_dicts)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim_model, T_max=150)
    total_step = len(train_loader)

    # evaluate1(args, val_loader, model, vocab, 0, '', tags_l)

    for epoch in range(args.num_epochs):
        model.train()
        met_t = np.zeros([len(tags_l), 4])
        t_tag_l, t_sent_l, t_word_l, t_l = 0, 0, 0, 0

        for i, (images, captions, prob, tags) in enumerate(train_loader):
            model.zero_grad()
            # l1_regularization = torch.tensor([0], dtype=torch.float32).to(args.device)

            batch_size = images.shape[0]
            images = images.to(args.device)
            captions = captions.to(args.device)
            prob = prob.to(args.device)
            tags = tags.to(args.device)

            tag_score, predict_tags, ps, word_outputs, pred_words = model(images, captions, args, 'train')

            met_t = metrics.cala(tags, predict_tags, met_t)

            tags_loss = criterion_fl(tag_score.view(-1, 2), tags.view(-1))

            loss_sent = criterion_stop(ps.view(-1, 2), prob.view(-1))

            loss_word = criterion_words(word_outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))

            loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag
            t_tag_l += tags_loss.item()
            t_sent_l += loss_sent.item()
            t_word_l += loss_word.item()
            t_l = t_tag_l + t_sent_l + t_word_l

            loss.backward()
            optim_model.step()

            # Print log info
            if i % args.log_step == 0:
                print('train： Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} >{:.2f}< >{:.2f}< >{:.5f}<'
                    .format(
                    epoch, args.num_epochs, i, total_step, loss.item(), t_sent_l * args.lambda_sent / (i + 1),
                                                                        t_word_l * args.lambda_word / (i + 1),
                                                                        t_tag_l * args.lambda_tag / (i + 1)))
        scheduler.step()
        scores = evaluate1(args, val_loader, model, vocab, epoch, title, tags_l)
        loss_t.line([[t_l / (i + 1), scores[0]]], [epoch], win='loss', update='append')
        scores_t.line([scores[1:]], [epoch], win='scores', update='append')
        # print('lr:' + str([round(j['lr'], 8) for j in optim_model.param_groups])[1:-1])
        print('train metrics', np.sum(met_t, axis=0), 'Acc Precision Recall F1---------------->>',
              metrics.acc_f1(met_t))
    for i in range(len(tags_l)):
        print(' ' * 8, tags_l[i], ' ' * (40 - len(tags_l[i])), metrics.acc_f1(met_t[i]), met_t[i])


def evaluate1(args, val_loader, model, vocab, epoch, title, tags_l):
    model.eval()
    met_e = np.zeros([len(tags_l), 4])
    vocab_size = len(vocab)
    total_step = len(val_loader)
    t_tag_l_v, t_sent_l_v, t_word_l_v, t_l_v = 0, 0, 0, 0
    # criterion_tags_val = nn.CrossEntropyLoss().to(args.device)
    criterion_stop_val = nn.CrossEntropyLoss().to(args.device)
    criterion_words_val = nn.CrossEntropyLoss().to(args.device)
    criterion_fl_val = focal_loss(alpha=args.fl_alpha, gamma=args.fl_gamma).to(args.device)
    # criterion_mse = nn.MSELoss().to(args.device)

    references = list()
    hypotheses = list()

    for i, (images, captions, prob, tags) in enumerate(val_loader):
        images = images.to(args.device)
        captions = captions.to(args.device)
        prob = prob.to(args.device)
        tags = tags.to(args.device)

        tag_score, predict_tags, ps, word_outputs, pred_words = model(images, captions, args, 'val')

        met_e = metrics.cala(tags, predict_tags, met_e)

        tags_loss = criterion_fl_val(tag_score.view(-1, 2), tags.view(-1))

        loss_sent = criterion_stop_val(ps.view(-1, 2), prob.view(-1))
        loss_word = criterion_words_val(word_outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))
        loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag
        t_tag_l_v += tags_loss.item()
        t_sent_l_v += loss_sent.item()
        t_word_l_v += loss_word.item()
        t_l_v = t_tag_l_v + t_sent_l_v + t_word_l_v

        for j in range(captions.shape[0]):
            pred_caption = []
            target_caption = []
            for k in range(captions.shape[1]):
                if ps[j, k, 1] > 0.5:
                    words_x = pred_words[j, k, :].tolist()

                    pred_caption.append(" ".join([vocab.idx2word[w] for w in words_x if
                                                  w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'],
                                                            vocab.word2idx['<end>']}]) + ".")

                if prob[j, k] == 1:
                    words_y = captions[j, k, :].tolist()

                    target_caption.append(" ".join([vocab.idx2word[w] for w in words_y if
                                                    w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'],
                                                              vocab.word2idx['<end>']}]) + ".")

            hypotheses.append(pred_caption)
            references.append(target_caption)
        if i % args.log_step == 0:
            print('val: Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} >{:.2f}< >{:.2f}< >{:.5f}<'
                .format(
                epoch, args.num_epochs, i, total_step, loss.item(), t_sent_l_v * args.lambda_sent / (i + 1),
                                                                    t_word_l_v * args.lambda_word / (i + 1),
                                                                    t_tag_l_v * args.lambda_tag / (i + 1)
            ))
    assert len(references) == len(hypotheses)
    print('eval metrics', np.sum(met_e, axis=0), 'Acc Precision Recall F1-->>', metrics.acc_f1(met_e))
    for k in range(len(tags_l)):
        print(' ' * 8, tags_l[k], ' ' * (40 - len(tags_l[k])), metrics.acc_f1(met_e[k]), met_e[k])
    print(references[0])
    print(hypotheses[0])

    if epoch == args.num_epochs - 1:
        datas = pandas.DataFrame(columns=['groud_truth', 'generation'])
        for j in range(len(references)):
            datas.loc[j] = [references[j], hypotheses[j]]
        datas.to_csv(
            '/home/mzjs/bio_image_caption/On-the-Automatic-Generation-of-Medical-Imaging-Reports/result/' + title + '.csv')
        print('Save to file: ', title, '.csv')

    blue1, blue2, blue3, blue4, meteor, rouge, cider = evalscores(hypotheses, references)
    return t_l_v / (i + 1), blue1, blue2, blue3, blue4, meteor, rouge, cider


def evaluate(args, val_loader, encoderCNN, mlc, sentLSTM, wordLSTM, vocab, epoch, title, tags_l):
    encoderCNN.eval()
    mlc.eval()
    sentLSTM.eval()
    wordLSTM.eval()
    met_e = np.zeros([len(tags_l), 4])
    vocab_size = len(vocab)
    total_step = len(val_loader)
    t_tag_l_v, t_sent_l_v, t_word_l_v, t_l_v = 0, 0, 0, 0
    # criterion_tags_val = nn.CrossEntropyLoss().to(args.device)
    criterion_stop_val = nn.CrossEntropyLoss().to(args.device)
    criterion_words_val = nn.CrossEntropyLoss().to(args.device)
    criterion_fl_val = focal_loss(alpha=args.fl_alpha, gamma=args.fl_gamma).to(args.device)
    # criterion_mse = nn.MSELoss().to(args.device)

    references = list()
    hypotheses = list()

    for i, (images, captions, prob, tags) in enumerate(val_loader):
        images = images.to(args.device)
        captions = captions.to(args.device)
        prob = prob.to(args.device)
        tags = tags.to(args.device)
        # l1_regularization_eval = torch.tensor([0], dtype=torch.float32).to(args.device)

        # test_caption = list()
        # print(prob[0])

        # for x in range(captions.shape[1]):
        # 	print(prob[0][x])
        # 	print("diff")
        # 	print(prob[0, x])
        # 	if prob[0, x] == 1:
        # 		test_words = captions[0, x, :].tolist()
        # 		test_caption.extend([w for w in test_words if w not in {vocab.word2idx['<pad>']}])

        # print([vocab.idx2word[k] for k in test_caption])

        vis_enc_out = encoderCNN(images)  # (batch_size, , vis_dim)

        sem_enc_out = vis_enc_out[:, :1, :]
        vis_enc_out = vis_enc_out[:, 1:, :]
        # sem_enc_out = vis_enc_out.mean(1).unsqueeze(1)

        tag_score, sem_enc_out, _2 = mlc(sem_enc_out, args.device)
        topics, ps = sentLSTM(vis_enc_out, sem_enc_out, captions, args.device)
        # topics, ps = sentLSTM(vis_enc_out, sem_enc_out, captions, args.device)
        met_e = metrics.cala(tags, _2, met_e)

        # for parm in mlc.parameters():
        #     l1_regularization_eval += torch.sum(abs(parm))  # calculation L1 regularization

        # tags_loss = criterion_tags_val(tag_score.view(-1, 2), tags.view(-1))
        tags_loss = criterion_fl_val(tag_score.view(-1, 2), tags.view(-1))
        # tags_loss = criterion_mse(tag_score.view(-1), tags.view(-1).float())
        loss_sent = criterion_stop_val(ps.view(-1, 2), prob.view(-1))

        loss_word = torch.tensor([0.0]).to(args.device)

        pred_words = torch.zeros((captions.shape[0], captions.shape[1], captions.shape[2]))

        for j in range(captions.shape[1]):
            word_outputs = wordLSTM(topics[:, j, :], captions[:, j, :])

            loss_word += criterion_words_val(word_outputs.contiguous().view(-1, vocab_size),
                                             captions[:, j, :].contiguous().view(-1))

            _, words = torch.max(word_outputs, 2)

            pred_words[:, j, :] = words

        # loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag \
        #        + args.lambda_l1 * l1_regularization_eval
        loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word + tags_loss * args.lambda_tag
        # loss = args.lambda_sent * loss_sent + args.lambda_word * loss_word
        t_tag_l_v += tags_loss.item()
        t_sent_l_v += loss_sent.item()
        t_word_l_v += loss_word.item()
        t_l_v = t_tag_l_v + t_sent_l_v + t_word_l_v

        for j in range(captions.shape[0]):
            pred_caption = []
            target_caption = []
            for k in range(captions.shape[1]):
                if ps[j, k, 1] > 0.5:
                    words_x = pred_words[j, k, :].tolist()

                    # pred_caption.extend([w for w in words_x if w not in {vocab.word2idx['<pad>'], vocab.word2idx[
                    # '<start>']}])

                    pred_caption.append(" ".join([vocab.idx2word[w] for w in words_x if
                                                  w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'],
                                                            vocab.word2idx['<end>']}]) + ".")

                if prob[j, k] == 1:
                    words_y = captions[j, k, :].tolist()
                    # target_caption.extend([w for w in words_y if w not in {vocab.word2idx['<pad>'], vocab.word2idx[
                    # '<start>']}])
                    target_caption.append(" ".join([vocab.idx2word[w] for w in words_y if
                                                    w not in {vocab.word2idx['<pad>'], vocab.word2idx['<start>'],
                                                              vocab.word2idx['<end>']}]) + ".")

            hypotheses.append(pred_caption)
            references.append(target_caption)
        if i % args.log_step == 0:
            print('val: Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} >{:.2f}< >{:.2f}< >{:.5f}<'
                .format(
                epoch, args.num_epochs, i, total_step, loss.item(), t_sent_l_v * args.lambda_sent / (i + 1),
                                                                    t_word_l_v * args.lambda_word / (i + 1),
                                                                    t_tag_l_v * args.lambda_tag / (i + 1)
            ))
    assert len(references) == len(hypotheses)
    print('eval metrics', np.sum(met_e, axis=0), 'Acc Precision Recall F1-->>', metrics.acc_f1(met_e))
    for i in range(len(tags_l)):
        print(' ' * 8, tags_l[i], ' ' * (40 - len(tags_l[i])), metrics.acc_f1(met_e[i]), met_e[i])
    print(references[0])
    print(hypotheses[0])

    if epoch == args.num_epochs - 1:
        datas = pandas.DataFrame(columns=['groud_truth', 'generation'])
        for j in range(len(references)):
            datas.loc[j] = [references[j], hypotheses[j]]
        datas.to_csv(
            '/home/mzjs/bio_image_caption/On-the-Automatic-Generation-of-Medical-Imaging-Reports/result/' + title + '.csv')
        print('Save to file: ', title, '.csv')

    blue1, blue2, blue3, blue4, meteor, rouge, cider = evalscores(hypotheses, references)
    return t_l_v / (i + 1), blue1, blue2, blue3, blue4, meteor, rouge, cider


# print([vocab.idx2word[references[0][0][k]] for k in range(len(references[0][0]))])
# print([vocab.idx2word[hypotheses[0][k]] for k in range(len(hypotheses[0]))])
# bleu4 = corpus_bleu(references, hypotheses)
# bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))
# bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))
# bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))


# print(
#     '\n * BLEU-1 - {bleu1}, BLEU-2 - {bleu2}, BLEU-3 - {bleu3}, BLEU-4 - {bleu4}\n'.format(
#         bleu1=bleu1,
#         bleu2=bleu2,
#         bleu3=bleu3,
#         bleu4=bleu4))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--root_dir', type=str, default='../SiVL19/iu_xray', help='path for root dir')
    parser.add_argument('--dataset', type=str, default='iuxray', help='path for root dir')
    parser.add_argument('--train_tsv_path', type=str, default='train1.json', help='path of the training tsv file')
    parser.add_argument('--val_tsv_path', type=str, default='val1.json', help='path of the validating tsv file')
    parser.add_argument('--image_path', type=str, default='iu_xray_images', help='path of the images file')
    parser.add_argument('--img_size', type=int, default=224, help='size to which image is to be resized')
    parser.add_argument('--crop_size', type=int, default=224, help='size to which the image is to be cropped')
    parser.add_argument('--device_number', type=str, default='1', help='which GPU to run experiment on')
    parser.add_argument('--use_cuda', type=bool, default=True, help='use cuda')
    parser.add_argument('--title', type=str, default='default', help='title')
    parser.add_argument('--seed', type=int, default=9224, help='seed of random')

    parser.add_argument('--int_stop_dim', type=int, default=64,
                        help='intermediate state dimension of stop vector network')
    parser.add_argument('--sent_hidden_dim', type=int, default=512, help='hidden state dimension of sentence LSTM')
    parser.add_argument('--sent_input_dim', type=int, default=1024, help='dimension of input to sentence LSTM')
    parser.add_argument('--word_hidden_dim', type=int, default=512, help='hidden state dimension of word LSTM')
    parser.add_argument('--word_input_dim', type=int, default=1024, help='dimension of input to word LSTM')
    parser.add_argument('--att_dim', type=int, default=64,
                        help='dimension of intermediate state in co-attention network')
    parser.add_argument('--num_layers', type=int, default=2, help='number of layers in word LSTM')

    parser.add_argument('--lambda_sent', type=int, default=1,
                        help='weight for cross-entropy loss of stop vectors from sentence LSTM')
    parser.add_argument('--lambda_word', type=int, default=1,
                        help='weight for cross-entropy loss of words predicted from word LSTM with target words')
    parser.add_argument('--lambda_tag', type=int, default=10,
                        help='weight for cross-entropy loss of tag')
    parser.add_argument('--lambda_l1', type=int, default=5e-7,
                        help='weight for l1 regularization')

    parser.add_argument('--fl_alpha', type=float, default=0.2, help='')
    parser.add_argument('--fl_gamma', type=int, default=3, help='')
    parser.add_argument('--batch_size', type=int, default=16, help='size of the batch')
    parser.add_argument('--shuffle', type=bool, default=True, help='shuffle the instances in dataset')
    parser.add_argument('--num_workers', type=int, default=0, help='number of workers for the dataloader')
    parser.add_argument('--num_epochs', type=int, default=100, help='number of epochs to train the model')
    parser.add_argument('--learning_rate_cnn', type=int, default=1e-5, help='learning rate for CNN Encoder')
    parser.add_argument('--learning_rate_mlc', type=int, default=1e-5, help='learning rate for MLC Encoder')
    parser.add_argument('--learning_rate_lstm', type=int, default=1e-5, help='learning rate for LSTM Decoder')

    parser.add_argument('--log_step', type=int, default=100, help='step size for prining log info')
    parser.add_argument('--save_step', type=int, default=1000, help='step size for saving trained models')

    args = parser.parse_args()
    # torch.cuda.empty_cache()
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device_number
    # os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    args.device = torch.device('cuda' if args.use_cuda else 'cpu')
    print('cuda version:', torch.version.cuda)
    print(args.title)

    setup_seed(args.seed)
    main(args)
